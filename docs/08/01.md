---
lang: zh-CN
title: Section 8.1 Storage Concepts
description: some description
---

As you study this section, answer the following questions:

- What are the common types of storage devices?
- How is the Linux file system tree different from Windows?
- What is block storage?

In this section, you will learn to:

- Use lsblk

The key terms for this section include:

<table class="terms">
<thead>
  <tr>
    <th>Term</th>
    <th>Definition</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Block storage</td>
    <td>
      Storage used by Linux to store traditional data in blocks or
      chunks of space (also called a block device).
    </td>
  </tr>
  <tr>
    <td>
      Master Boot Record
      <br />
      (MBR)
    </td>
    <td>The traditional partition type used for storage devices.</td>
  </tr>
  <tr>
    <td>Globally Unique Identifier (GUID) Partition Table (GPT)</td>
    <td>
      The successor to MBR partition tables. It provides much more
      storage capability and partition flexibility.
    </td>
  </tr>
</tbody>
</table>

This section helps you prepare for the following certification exam objectives:

<table class="objectives">
<thead>
<tr>
  <th>Exam</th>
  <th>Objective</th>
</tr>
</thead>
<tbody>
<tr>
  <td>TestOut Linux Pro</td>
  <td>
    2.1 Manage storage devices
    <ul>
      <li>Create and manage disk partitions</li>
    </ul>
  </td>
</tr>
<tr>
  <td>CompTIA Linux+ XK0-005</td>
  <td>
    1.1 Summarize Linux fundamentals
    <ul>
      <li>
        Storage concepts
        <ul>
          <li>File storage</li>
          <li>Block storage</li>
          <li>Object storage</li>
          <li>Partition type</li>
          <li>FUSE</li>
          <li>
            RAID
            <ul>
              <li>Striping</li>
              <li>Mirroring</li>
              <li>Parity</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    1.3 Configure and manage storage using appropriate tools
    <ul>
      <li>
        Storage area network (SAN) / network-attached storage (NAS)
        <ul>
          <li>multipathd</li>
          <li>
            Network filesystems
            <ul>
              <li>Network File Systems (NFS)</li>
              <li>
                Server Message Block/Common Internet File System
                (CIFS)
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <p></p>
  </td>
</tr>
</tbody>
</table>

## 8.1.1 Storage Types

Click one of the buttons to take you to that part of the video.

Storage Types 00:00-01:09
A computer system without storage is like a desert without sand. We have a box of electronics, but nothing that makes it work. Storage is the lifeblood of a computer system. It holds the operating system, programs, files, and data. In this lesson, we'll discuss the various types of storage, their differences, and how to utilize them.

Storage devices vary in size, shape, and use. There are two basic categories for storage: internal and external. Internal storage is inside the computer case. External means the storage is elsewhere—usually accessed via a network connection. Most systems have some sort of internal storage that contains local data requirements, such as an operating system, applications, and local data. External storage may contain common applications and shared data. Internal storage includes a magnetic, optical, and solid state. External storage may consist of the same type of storage devices. However, it's managed separately and usually has a much higher capacity than local storage—often in the hundreds of terabytes. This storage is accessed via networked devices such as SAN or NAS.

Local Storage 01:09-03:26
Inside most computer systems, there's some kind of internal storage with at least the system's operating system installed and configured. The first type is the magnetic or rotational drive. Magnetic hard drives were the first type of mass storage available for microcomputers and have remained since PCs have used mass storage. While they're still used, they're being replaced with flash-based solid state drives, or SSD. SSDs are more common today than magnetic drives due to their speed for storing and retrieving data. They're more expensive than magnetic storage and don't have the capacity that magnetic has. However, they're still a better choice for most PCs and notebooks. Solid state also comes in a different form-factor known as M2 or its update, non-volatile memory express, or NVMe.

These storage types provide the ability to write and read data. Another type of data storage is optical. Optical storage can be thought of as write-once-read-many or WORM. While there are media available that provide for erasing and reusing optical storage, WORM drives are much more common. DVD and BluRay movies are examples of optical media. Optical drives were once the primary method for transporting data. Now, thumb drives or USB drives are more popular since they're much less expensive, very easy to use, and are available in much higher capacities than optical.

When adding storage to a local system, there may be a requirement to manage large data stores. Many systems have the ability to manage multiple storage devices, and Linux provides a method for this management called Linux Volume Management or LVM. Suppose you had a system with four physical storage devices. Via Linux, you create a partition on each device that spans the entire device, which becomes the physical volume. Via LVM management tools, you create a Logical volume group or VG by combining the space available on the physical volumes into a pool of storage space. With the VG in place, you can now create Logical volumes or LVs and format them with a Linux filesystem for general use. You simply configure an LV with the amount of space you wish to consume from the space available on the VG, define the Linux mount point, and format the LV with BtrFS, ext4, or any other available Linux file system. Once formatted, the LV is ready to use.

RAID 03:26-06:46
Depending on whom you talk to, you may get a different definition of RAID. Most will say RAID is an acronym for Redundant Array of Independent Disks. There are several different array types designated by a number. Each number designates a different type of array that performs a different function. All RAID numbers provide a model for storage that's handled a bit differently than a single storage device could. Some add capacity, and others add redundancy. The following is a partial list of common RAID levels:

RAID 0 is striping. The array's storage capacity is the sum of all storage devices in the array. This means no additional cost or storage loss is using this RAID level. Data is written and read across all drives in the array, making it the fasted array in RAID. The problem is that there's no redundancy, so if a single drive fails, the entire array fails. Backups are very important. RAID 1 is mirroring. This is a costly level since the array's storage capacity is halved. It requires two drives, but only provides the capacity of a single drive. This level provides redundancy. Data is written to both drives simultaneously, causing a small performance delay. This delay is often imperceptible, and reads often see a performance increase. The benefit is redundancy. RAID 1 often provides fault tolerance to a server's boot disk. Should one of the two drives fail, the other takes over without skipping a beat.

One of the most popular RAID levels is RAID 5. RAID 5 is striping with parity. Data is striped, just like RAID 1, across all of the drives in the array. The difference between RAID 1 and RAID 5 is parity. This means each drive reserves a portion of its capacity to store information about the other drives. Parity reduces overall capacity but adds redundancy. If a single drive in the array fails, the surviving drives use their parity to take its place.

Suppose you have three 8 TB drives in your system. For striping, we combine the space of all the drives giving us a 24 TB capacity. We have to add parity which takes away from the total capacity. Each drive's capacity is reduced by a fraction equal to 1 divided by the number of drives in the array. In this example, we have three drives, so we must reduce the capacity of 1 divided by 3 or one-third. Another way to measure the lost capacity is to remove the capacity of a single drive from the array. This calculation provides the total available capacity of the RAID 5 array.

Should a single drive fail, the others will utilize their parity to keep the array running. Should this happen, the array will be in a degraded state until the failed drive is replaced and the RAID rebuilds the drive.

RAID 6 is similar to RAID 5 as it's striping with parity. The difference is that RAID 6 uses double parity and can withstand a loss of 2 drives from the array. The net capacity of the RAID 6 array is the total capacity minus the capacity of 2 drives.

There are other RAID levels that combine the ones already discussed. RAID 1+0 or RAID 10 is a mirror of stripes, and RAID 0+1 or RAID 01 is a stripe of mirrors.

The RAID levels available to you are defined by the RAID controller in your system. Several vendors manufacture RAID controllers, and some are proprietary. Consult your RAID controller's implementation guide to find out which RAID levels are supported by your system. Additionally, Linux LVM provides the ability to utilize software to create RAID levels, such as mirroring or striping with parity.

External Storage 06:46-07:49
So far, we've discussed internal storage. Now, we'll discuss external storage. Simply put, external storage is managed storage available using networking and network protocols. It's often a higher capacity found in most internal systems and is available to devices connected to an internal network. The storage may be in the same room, building, or halfway around the world.

One of the more common external storage methods is Network Attached Storage or NAS. As the name implies, the storage is connected to the network and is accessed using network protocols. Your system may access the data via the data LAN or have specific network cards that are specifically connected to a storage network. Most often, NAS uses shares similar to an MS Windows or a Linux share and provides access to users. Another method is the Storage Area Network or SAN. A SAN provides all the connectivity, storage, and control to systems. Generally, a SAN provides external storage to servers, including diskless servers. There's a direct connection, usually fiber optic, from the server to the storage array.

iSCSI 07:49-09:25
There's another method for connecting servers to a SAN. iSCSI provides another connectivity option using a standard Ethernet infrastructure. iSCSI sends commands to the SAN using Ethernet for transport rather than a direct fiber optic connection. The iSCSI initiator sends SCSI commands to the iSCSI target, and the target provides the requested data. Often a separate storage Ethernet network is created for iSCSI communications using higher-speed Ethernet devices such as 10 Gbps or faster. Linux can operate as an iSCSI target and iSCSI initiator. Depending on your distribution, you may have to add the iSCSI components.

There are a few steps needed to establish connectivity to an iSCSI target. Remember, we're using Ethernet, so we have to define the device we're connecting to. The first step is ensuring you have the iSCSI initiator tools for your distribution. One example is shown here. After the tools are installed, consult the tools manual for the correct usage of the tools to connect to the iSCSI target. The method shown here is for a specific distribution. Your method may differ. Once you have the tools, you need to find your initiator's name. We need to find the iSCSI-qualified name or IQN for the iSCSI target. We need to know its IP address and send it a query to find its name. Here we use the ISCSI administrator tool to send a discovery for the send target type at the IP address listed. With the IQN, we can now connect to the target. We need to look in the messages database to find the connected iSCSI device name. Now, we have to format the device. Once the device is formatted, it can be mounted to the local filesystem.

Summary 09:25-09:46
That's all we have for this lesson. In this lesson, you learned about different storage types. The first type was internal storage. You then learned about RAID and RAID levels. Next, we discussed external storage, NAS, and SAN. Finally, we discussed iSCSI and showed connecting an initiator to a target.

## 8.1.2 FUSE

Click one of the buttons to take you to that part of the video.

Filesystem in Userspace 00:00-00:17
In this lesson, we're going to look at creating a virtual file system, or VFS, in the user space. File systems built in the user space are known as FUSE. Let's quickly review the importance of file systems.

What is FUSE? 00:17-01:24
Hard drives are large unallocated disks used for storing data. Alone they are not very useful. When using file systems to organize and save our data, the computer saves our files, folders, and configurations. Also, users can move and save files. File systems are very useful but come with a few issues.

First, typically, only users with administrative access are allowed to make changes to the file system and protected portions of the hard drive. They are also the only users that can mount and unmount different hard drives or partitions.

Second, file systems can be large and complex to navigate. With larger hard drives available, file systems continue to grow.

Modern systems use RAIDs, which spreads the file system across multiple drives and makes the system even more complex.

Last, file systems live in the kernel space for operating systems, meaning the OS is responsible for managing the file system. This can make debugging file system issues difficult. And while debugging, we have a greater chance of crashing the machine.

Enter FUSE.

FUSE 01:24-01:53
FUSE stands for file system in user space. The idea is that we set aside portions of the file system in use by users to create a virtual file system, or VFS. Once this portion of the file system is set aside, we create a FUSE kernel module named fuse.ko and insert it into the kernel.

This creates a tool that users can use to load or mount the virtual file system without needing administrative privileges.

Parts of FUSE 01:53-02:27
In order to use FUSE, you'll need 3 elements installed on your Linux System. These elements typically need to be installed by an Administrator.

First, a kernel module that can be used to load the FUSE VFS.

Second, a user space library to interact with the FUSE VFS. This is typically one of the libfuse packages.

Third, a mount utility to give the user permissions to mount, as well as allow the system to mount, the FUSE VFS.

Now, let's take a look at the advantages and purpose of using FUSE.

Purpose of FUSE 02:27-03:11
FUSE gives users more control over file operations. It provides a way for non-privileged users to create and mount file systems, restrict access, and give services or programs full permissions to entire directories or files.

The file system is not managed by the kernel, but rather opened by the kernel. Because of this we can easily port or move the file system between machines, or make it available to resources such as containers or virtual machines.

Lastly, the fuse.ko kernel module is used to access the FUSE system, but doesn't contain the actual file system. Bugs in the VFS won't affect the kernel, which provides a stable system environment.

Summary 03:11-03:26
In this lesson, we talked about the file system in user space or FUSE. We looked at the requirements in order to run FUSE and the purpose of creating FUSE virtual file systems.

## 8.1.3 RAID on Linux

Click one of the buttons to take you to that part of the video.

RAID on Linux 00:00-00:16
In this lesson, we're going to look at how Linux builds and manages RAID. Linux typically creates software RAIDs, and these are installed on the Linux kernel and loaded by the boot loader. Let's take a closer look at how this works.

mdadm 00:16-00:47
Linux builds software RAIDs with the multiple device administration tool, or mdadm. Let's say that we have five hard drives. Each drive will automatically be assigned a drive name, beginning with sda. To create a software RAID, Linux builds a new disk device that begins with md, which is short for multiple device. Then it gets assigned a device number, usually starting with zero. Now, if we want to use this RAID as a boot device for Linux, we need to set aside partitions on one of the drives to be used as the /boot directory.

Configure the Boot Disk 00:47-01:13
So let's look at our first hard disk device, sda. It'll be split into three partitions. Each one will receive the device name, sda, followed by which partition number it is. First, we need to set aside 1 megabyte to be a BIOS grub spacer, and we need to create a partition to house the /boot directory. The remaining space will be used as a RAID component.

Once the boot device is configured, we can finish building the RAID.

Complete the RAID 01:13-01:52
Partition 3, which is /dev/sda3 on Disk 1, and the remaining 4 disks—sdb, sdc, sdd, and sde—are the ones we'll use. They'll be marked as components of the new RAID, which is md0, and be used to create the new device, /dev/md0.

Here, we have a sample of how a new RAID 5 will look on an Ubuntu server. You can see the md0 device that was created and how each disk is marked as a component of the software RAID. You can also see our BIOS grub spacer, the /boot directory, and the partition that'll be used as a component of the RAID.

RAID Types 01:52-02:16
The mdadm utility can be used to create most of the RAID types you'll need. This includes RAID 0, which splits the data across two or more hard drives, and RAID 1, which copies the data from one drive to another. There's also RAID 5, which stripes data across three or more drives while providing redundancy with parity. And RAID 10 takes RAID 5 and mirrors the data to another RAID 5.

Summary 02:16-02:33
That's all for this lesson. We learned about creating software RAIDs in Linux. We reviewed the mdadm utility and looked at how to configure a boot disk within a RAID. We also briefly reviewed common RAID types.

## 8.1.4 Storage Facts

This lesson covers the following topics:

- Linux storage concepts
- FUSE
- RAID on Linux

### Linux Storage Concepts

Linux is an operating system with roots in many historical computing environments. When discussing storage on Linux, we need to understand the ancestry of some of the concepts in order to make sense of how they are implemented on Linux.

The layout of the files and folders on Linux is, depending on the distribution, determined loosely or tightly by the File system Hierarchy Standard (FHS). This standard defines where files and folders are stored, based on their function. For example, the `/home` directory is where a standard user keeps their personal files. This standard also includes definitions where types of storage are generally located. The process of attaching storage devices in Linux is called mounting . Thus, directly attached storage that is used as the root of the file system is found in the `/` directory, which is called the root directory.

On Linux systems, all mounted storage devices are attached to the same file system somewhere below the / location. This idea of a single tree—instead of a number of trees as found under Windows such as `C:\`, `D:\`, etc. `—comes` from the Unix world. An optical device that has a data DVD inserted by a user will typically be found under `/run/media/<username>`. A USB storage device, such as a thumb drive, will also be found under `/run/media/<username>` when inserted by a user.

There is a mount command that is used to do two primary things: list all file systems that are currently mounted and allow a privileged user to mount a file system on a storage device somewhere in the root file system tree. Although the mount command is used by users occasionally, most file systems are mounted automatically at boot or mounted when they are added to the system.

### cd

Since this lesson is not about all of the storage types or the various ways of mounting or configuring the storage, we'll keep our focus on two of the most commonly used types of storage: FUSE and RAID. One of the types of storage that is used but won't be covered in detail is Fibre Channel . It is used in high-speed storage environment Storage Area Networks (SANs), and the fcstat command is used to gather information about fibre channel configuratoins.

### Storage Types

There are several different ways in which data is stored on a Linux system. These are described by the manner in which the data is organized on the devices:

<ul><li>File storage - This method is used by services such as NFS and SMB/CIFS for storage of files over the network, although locally attached storage devices also can use this storage type.</li>
<li>Block storage - This is the oldest and most common type of storage, where data is placed in fixed length blocks of data. This is commonly used for hosting the operating system, applications and databases, and 
local data storage.</li>
<li>Object storage - The newest method for storing data, object storage makes data available to clients in their original form, usually accessed in the form of a URL.</li></ul>

### FUSE

The Filesystem in USErspace (FUSE) project was built as a way for regular, non-privileged users to create file systems without affecting the kernel.

The primary use for FUSE is for creating virtual file systems for applications. Specifically, sandboxed applications such as AppImages use FUSE to create a restricted, disconnected from the kernel, file system. These isolated file systems leave kernel access to the FUSE kernel module, keeping the application from compromising system security even if they have vulnerabilities in them. While this approach is not always effective, it does provide another layer of security.

### Network File Systems

In addition to storage attached directly to a server, storage devices can be located on another host on the network that shares its storage space with other network hosts. Using network file systems, data can be written to the remote location as if attached locally, at least from the user's perspective. There are two primary network file systems that are used in Linux:

<ul><li>Network File System (NFS) - NFS is a protocol used by servers and clients to share storage on a network. It comes from the Unix world and has been in use since 1984.</li>
<li>Server Message Block (SMB)/Common Internet File System (CIFS) - These protocols describe how to share storage across the network, much like NFS. The core protocols are used by Microsoft Windows for storage 
sharing in a Windows environment, which Linux can participate in with some limitations.</li></ul>

### Multipathing Storage

One of the common ways redundancy for storage is created is using multipathing. Using multiple physical connections between a server and a storage array, such as Storage Area Network (SAN) or Network Attached Storage (NAS), data can be written to the target storage device when one of the paths becomes unavailable, such as what might be caused by a hardware failure. Linux uses the multipath daemon called multipathd to manage the behavior of the data being written to the storage array(s) in such a way as to provide redundancy in the case of a failure along one path.

### RAID on Linux

Redundant Array of Independent Disks (RAID), also called Redundant Array of Inexpensive Disks, is a disk subsystem that combines multiple physical disks into a single logical storage unit. Depending on the configuration, a RAID array can improve performance, provide fault tolerance, or both.

The following table describes common RAID levels.

<table>
<thead>
  <tr>
    <th>RAID Level</th>
    <th>Description</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>RAID 0 (striping)</td>
    <td>
      A stripe set breaks data into units and stores the units across a
      series of disks by reading and writing to all disks
      simultaneously. Striping:
      <ul>
        <li>Provides an increase in performance.</li>
        <li>
          Does not provide fault tolerance. A failure of one disk in the
          set means all data is lost.
        </li>
        <li>Requires a minimum of two disks.</li>
        <li>
          Has no overhead because all disk space is available for
          storing data.
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>RAID 1 (mirroring)</td>
    <td>
      A mirrored volume stores data to two (or more) duplicate disks
      simultaneously. If one disk fails, data is present on another
      disk. The system switches immediately from the failed disk to a
      functioning disk. Mirroring:
      <ul>
        <li>Provides fault tolerance for a single disk failure.</li>
        <li>Does not increase performance.</li>
        <li>Requires a minimum of two disks.</li>
        <li>
          Has overhead. Overhead is 1 / n where n is the number of
          disks. If data is written twice, half of the disk space is
          used to store the second copy of the data.
        </li>
        <li>RAID 1 is the most expensive fault tolerant system.</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>RAID 5 (striping with distributed parity)</td>
    <td>
      A RAID 5 volume combines disk striping across multiple disks with
      parity for data redundancy. Parity information is stored on each
      disk. If a single disk fails, its data can be recovered using the
      parity information stored on the remaining disks. Striping with
      distributed parity:
      <ul>
        <li>Provides fault tolerance for a single disk failure.</li>
        <li>
          Provides an increase in performance for read operations. Write
          operations are slower with RAID 5 than with other RAID
          configurations because of the time required to compute and
          write the parity information.
        </li>
        <li>Requires a minimum of three disks.</li>
        <li>
          Has an overhead of one disk in the set for parity information
          (1 / n).
          <ul>
            <li>A set with 3 disks has 33% overhead.</li>
            <li>A set with 4 disks has 25% overhead.</li>
            <li>A set with 5 disks has 20% overhead.</li>
          </ul>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>RAID 10 (stripe of mirrors)</td>
    <td>
      A RAID 10 volume stripes data across mirrored pairs and across
      multiple disks for data redundancy. If a single disk fails, its
      data can be recovered using the mirrored information stored on the
      remaining disks. If two disks in the same mirrored pair fail, all
      data will be lost because there is no redundancy in the striped
      sets. RAID 10:
      <ul>
        <li>Provides fault tolerance for a single disk failure.</li>
        <li>Provides redundancy and performance.</li>
        <li>
          Uses 50% of the total raw capacity of the drives due to
          mirroring.
        </li>
        <li>Requires a minimum of four disks.</li>
      </ul>
    </td>
  </tr>
</tbody>
</table>

Be aware of the following facts about RAID:

<ul><li>Some RAID controllers support combined levels of RAID. For example, RAID 0+1 is a striped array that is mirrored. Other combined configurations that might be supported include RAID 1+0 (also called RAID 10), RAID 5+0, and RAID 5+1.</li>
<li>For all RAID configurations, the amount of disk space used on each disk must be of equal size. If disks in the array are of different sizes, the resulting volume will be limited to the smallest disk. Remaining space on other drives can be used in other RAID sets or as traditional storage.</li>
<li>While some RAID configurations provide fault tolerance in the event of a disk failure, configuring RAID is not a substitute for regular backups.</li></ul>

On Linux, RAID arrays are created and managed from the command line with the mdadm (multi-disk administration) tool. Using this tool, an administrator can:

<ul><li>Create a new raid array. In this case, two storage devices are associated with the /dev/md0 RAID array device:
<ul><li><b >sudo mdadm --create --verbose /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</b></li></ul></li>
<li>Save the details of the new array to a file for configuration at boot:
<ul><li><b >sudo mdadm --detail --scan | sudo tee -a /etc/mdadm/mdadm.conf</b></li></ul></li></ul>

The most common way to monitor a RAID array on Linux is to read the status of a file in the /proc directory:

<ul><li><b>cat /proc/mdstat</b></li></ul>

![proc-mdstat output](/images/08/cat-raid.png)
